The task is topt!
We use CUDA!
epoch: 0/30;  rmse test: 0.195933; r2 test: -0.593594
epoch: 5/30;  rmse test: 0.18819; r2 test: -0.470129
epoch: 10/30;  rmse test: 0.137437; r2 test: 0.215902
epoch: 15/30;  rmse test: 0.134093; r2 test: 0.253596
epoch: 20/30;  rmse test: 0.116702; r2 test: 0.434651
epoch: 25/30;  rmse test: 0.113985; r2 test: 0.460663
Done.
1618
Topt Done!
The task is tm!
We use CUDA!
Traceback (most recent call last):
  File "run_train.py", line 196, in <module>
    train_result = train_eval( M , train_pack, test_pack , dev_pack, device, lr, batch_size, lr_decay,\
  File "run_train.py", line 67, in train_eval
    emb = esm2_model(batch_tokens, repr_layers=[6], return_contacts=False)
  File "/apps/system/easybuild/software/PyTorch/1.7.1-fosscuda-2020b/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/bras5181/.local/lib/python3.8/site-packages/esm/model/esm2.py", line 112, in forward
    x, attn = layer(
  File "/apps/system/easybuild/software/PyTorch/1.7.1-fosscuda-2020b/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/bras5181/.local/lib/python3.8/site-packages/esm/modules.py", line 125, in forward
    x, attn = self.self_attn(
  File "/apps/system/easybuild/software/PyTorch/1.7.1-fosscuda-2020b/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/bras5181/.local/lib/python3.8/site-packages/esm/multihead_attention.py", line 371, in forward
    attn_weights = attn_weights.masked_fill(
RuntimeError: CUDA out of memory. Tried to allocate 17.02 GiB (GPU 0; 44.48 GiB total capacity; 17.71 GiB already allocated; 16.41 GiB free; 27.00 GiB reserved in total by PyTorch)
751
Tm Done!
